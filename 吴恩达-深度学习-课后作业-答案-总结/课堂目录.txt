_第一周-深度学习概论-1.1-欢迎
_第一周-深度学习概论-1.2-什么是神剧网络
_第一周-深度学习概论-1.3-用神经网络进行监督学习
_第一周-深度学习概论-1.4-为什么深度学习会兴起
_第一周-深度学习概论-1.5-关于这门课程
_第一周-深度学习概论-1.6-课程资源

_第二周-神经网络基础-2.1-二分类
_第二周-神经网络基础-2.2-logistic回归
_第二周-神经网络基础-2.3-logistic回归损失函数
_第二周-神经网络基础-2.4-梯度下降法
_第二周-神经网络基础-2.5-导数
_第二周-神经网络基础-2.6-更多导数的例子
_第二周-神经网络基础-2.7-计算图
_第二周-神经网络基础-2.8-计算图的导数计算
_第二周-神经网络基础-2.9-logistic回归的梯度下降法
_第二周-神经网络基础-2.10-m个样本的梯度下降
_第二周-神经网络基础-2.11-向量化
_第二周-神经网络基础-2.12-向量化的更多例子
_第二周-神经网络基础-2.13-向量化logistic回归
_第二周-神经网络基础-2.14-向量化logistic回归的梯度输出
_第二周-神经网络基础-2.15-python中的广播
_第二周-神经网络基础-2.16-关于python/numpy向量的说明
_第二周-神经网络基础-2.17-jupyter/Ipython笔记本的快速指南
_第二周-神经网络基础-2.18-logistic损失函数的解释

_第三周-浅层神经网络-3.1-神经网络概览
_第三周-浅层神经网络-3.2-神经网络表示
_第三周-浅层神经网络-3.3-计算神经网络的输出
_第三周-浅层神经网络-3.4-多个例子中的向量化
_第三周-浅层神经网络-3.5-向量化实现的解释
_第三周-浅层神经网络-3.6-激活函数
_第三周-浅层神经网络-3.7-为什么需要非线性激活函数
_第三周-浅层神经网络-3.8-激活函数的导数
_第三周-浅层神经网络-3.9-神经网络的梯度下降
_第三周-浅层神经网络-3.10-（选修）直观理解反向传播
_第三周-浅层神经网络-3.11-随机初始化

_第四周-深层神经网络-4.1-深层神经网络
_第四周-深层神经网络-4.2-深层网络的前向传播
_第四周-深层神经网络-4.3-核对矩阵的维数
_第四周-深层神经网络-4.4-为什么使用深层表示
_第四周-深层神经网络-4.5-搭建深层神经网络块
_第四周-深层神经网络-4.6-前向和反向传播
_第四周-深层神经网络-4.7-参数VS超参数
_第四周-深层神经网络-4.8-这和大脑有什么关系

_第一周-深层学习的实用-1.1-训练/开发/测试集
_第一周-深层学习的实用-1.2-偏差/方差
_第一周-深层学习的实用-1.3-机器学习基础
_第一周-深层学习的实用-1.4-正则化
_第一周-深层学习的实用-1.5-为什么正则化可以减少过拟合
_第一周-深层学习的实用-1.6-DropOut正则化
_第一周-深层学习的实用-1.7-理解DropOut
_第一周-深层学习的实用-1.8-其他正则化方法
_第一周-深层学习的实用-1.9-正则化输入
_第一周-深层学习的实用-1.10-梯度小时与梯度爆炸
_第一周-深层学习的实用-1.11-神经网络的权重初始化
_第一周-深层学习的实用-1.12-梯度的数值逼近
_第一周-深层学习的实用-1.13-梯度校验
_第一周-深层学习的实用-1.14-关于梯度校验实现的标记

_第二周-优化算法-2.1-Minibath梯度下降法
_第二周-优化算法-2.2-理解Minibath梯度下降法
_第二周-优化算法-2.3-指数加权平均
_第二周-优化算法-2.4-理解指数加权平均
_第二周-优化算法-2.5-指数指数加权平均的偏差修正
_第二周-优化算法-2.6-动量梯度下降法
_第二周-优化算法-2.7-RMSprop
_第二周-优化算法-2.8-Adam 优化算法
_第二周-优化算法-2.9-学习率衰减
_第二周-优化算法-2.10-局部最优问题

_第三周-超参数调试&正则化&框架-3.1-调试处理
_第三周-超参数调试&正则化&框架-3.2-为超参数选择合适的范围
_第三周-超参数调试&正则化&框架-3.3-超参数训练的实践:pandas VS Caviar
_第三周-超参数调试&正则化&框架-3.4-正则化网络的激活函数
_第三周-超参数调试&正则化&框架-3.5-将Batch Norm拟合进神经网络
_第三周-超参数调试&正则化&框架-3.6-Batch Norm为什么凑效
_第三周-超参数调试&正则化&框架-3.7-测试时的Batch Norm
_第三周-超参数调试&正则化&框架-3.8-Softmax回归
_第三周-超参数调试&正则化&框架-3.9-训练一个Softmax分类器
_第三周-超参数调试&正则化&框架-3.10-深度学习框架
_第三周-超参数调试&正则化&框架-3.11-TensorFlow


_第一周-机器学习策略上-1.1-什么是ML策略
_第一周-机器学习策略上-1.2-正交化
_第一周-机器学习策略上-1.3-单一数字评估指标
_第一周-机器学习策略上-1.4-满足和优化指标
_第一周-机器学习策略上-1.5-训练/开发/测试集划分
_第一周-机器学习策略上-1.6-开发集和测试集的大小
_第一周-机器学习策略上-1.7-什么时候该改变开发集/测试集和指标
_第一周-机器学习策略上-1.8-为什么是人的表现
_第一周-机器学习策略上-1.9-可避免偏差
_第一周-机器学习策略上-1.10-理解人的表现
_第一周-机器学习策略上-1.11-超过人的表现
_第一周-机器学习策略上-1.12-改善你的模型表现

_第二周-机器学习策略下-2.1-进行误差分析
_第二周-机器学习策略下-2.2-清楚标记错误的数据
_第二周-机器学习策略下-2.3-快速搭建你的第一个系统并迭代
_第二周-机器学习策略下-2.4-在不同数据划分上进行训练并测试
_第二周-机器学习策略下-2.5-不匹配数据划分的偏差和方差
_第二周-机器学习策略下-2.6-定位数据不匹配
_第二周-机器学习策略下-2.7-迁移学习
_第二周-机器学习策略下-2.8-多任务学习
_第二周-机器学习策略下-2.9-什么是端到端的深度学习
_第二周-机器学习策略下-2.10-是否要使用端到端的深度学习

_第一周-卷积神经网络-1.1-计算机视觉
_第一周-卷积神经网络-1.2-边缘检测示例
_第一周-卷积神经网络-1.3-更多边缘检测内容
_第一周-卷积神经网络-1.4-Padding
_第一周-卷积神经网络-1.5-卷积步长
_第一周-卷积神经网络-1.6-卷积中"卷"的体现之处
_第一周-卷积神经网络-1.7-单层卷积网络
_第一周-卷积神经网络-1.8-简单卷积网络示例
_第一周-卷积神经网络-1.9-池化层
_第一周-卷积神经网络-1.10-卷积神经网络示例
_第一周-卷积神经网络-1.11-为什么使用卷积？

_第二周-深层卷积神经网络实例探究-2.1-为什么要进行实例探究
_第二周-深层卷积神经网络实例探究-2.2-经典网络
_第二周-深层卷积神经网络实例探究-2.3-残差网络
_第二周-深层卷积神经网络实例探究-2.4-残差网络为什么有用?
_第二周-深层卷积神经网络实例探究-2.5-网络中的网络以及1X1网络
_第二周-深层卷积神经网络实例探究-2.6-谷歌Inception网络简介
_第二周-深层卷积神经网络实例探究-2.7-Inception网络
_第二周-深层卷积神经网络实例探究-2.8-使用开源的实现方案
_第二周-深层卷积神经网络实例探究-2.9-迁移学习
_第二周-深层卷积神经网络实例探究-2.10-数据扩充
_第二周-深层卷积神经网络实例探究-2.11-计算机视觉现状

_第三周-目标检测-3.1-目标定位
_第三周-目标检测-3.2-特征点检测
_第三周-目标检测-3.3-目标检测
_第三周-目标检测-3.4-卷积的滑动窗口实现
_第三周-目标检测-3.5-BoundingBox预测
_第三周-目标检测-3.6-并交化
_第三周-目标检测-3.7-非极大值抑制
_第三周-目标检测-3.8-Anchor Boxes
_第三周-目标检测-3.9-YOLO算法
_第三周-目标检测-3.10-(选修)RPN网络

_第四周-人脸识别和神经风格转换-4.1-什么是人脸识别？
_第四周-人脸识别和神经风格转换-4.2-One-Shot学习
_第四周-人脸识别和神经风格转换-4.3-Siamese网络
_第四周-人脸识别和神经风格转换-4.4-Triplet损失
_第四周-人脸识别和神经风格转换-4.5-面部特征与二分类
_第四周-人脸识别和神经风格转换-4.6-什么是神经风格转换
_第四周-人脸识别和神经风格转换-4.7-深度卷积网络在学什么
_第四周-人脸识别和神经风格转换-4.8-代价函数
_第四周-人脸识别和神经风格转换-4.9-内容代价函数
_第四周-人脸识别和神经风格转换-4.10-风格损失函数
_第四周-人脸识别和神经风格转换-4.11-一维到三维推广


