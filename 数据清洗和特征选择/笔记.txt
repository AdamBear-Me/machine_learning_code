【马尔可夫模型】
统计模型，广泛应用在语音识别，词性自动标注，音字转换，概率文法等各个自然语言处理等应用领域
1、直接算法（暴力算法） 算出所有可能
2、前向算法
3、后向算法
后两个算法没懂
隐马尔可夫模型的学习根据训练数据的丌同分为监督学习和非监督学习。
若训练数据包括观测序列和对应的状态序列，则可迚行监督学习；
若训练数据仅有观测序列，那就要用非监督学习了

文章相关：
如何用简单易懂的例子解释隐马尔可夫模型？
https://www.zhihu.com/question/20962240
HMM相关文章索引
http://url.cn/5E2hMHv

---

【LDA(Latent Dirichlet Allocation)主题模型】
LDA是一个简单的模型，用来做主题分析（文本聚类），而不是用来生成文章的
LDA认为某一篇文章的生产过程是：
1、确定主题和词汇的分布
2、确定文章和主题的分布
3、随机确定该文章的词汇个数N
4、如果当前生成的词汇个数小于N执行第5步，否则执行第6步
5、由文档和主题分布随机生成一个主题，通过该主题由主题和词汇分布随机生成一个词，继续执行第4步
6、文章生成结束

上面的过程涉及的问题是：如何确定主题和词汇分布，还有文档与词汇的分布。
LDA说：【主题和词汇的分布就是多项式分布！！！！文章和主题之间的分布也是符合多项式分布！！！】
ps：反推的过程有点像贝叶斯后验

伯努利——二项分布——多项分布——贝塔分布——狄利克雷分布
LDA：http://blog.csdn.net/aws3217150/article/details/53840029【这个文档挺好，很重要！！！】

贝塔分布：https://baike.baidu.com/item/狄利克雷分布/12728892
狄利克雷分布：https://baike.baidu.com/item/%E8%B4%9D%E5%A1%94%E5%88%86%E5%B8%83/8994021
贝叶斯学习及共轭先验：http://blog.csdn.net/acdreamers/article/details/45026459
【两个概率分布如果具有相同的形式，我们就说它们是共轭的】

---

【贝叶斯网络】
LDA的文章-主题-词汇 就是一个三层贝叶斯网络
由于文章、主题和词汇不是独立分布的，它们之间有关联，所以不适用于朴素贝叶斯，然后引出了贝叶斯网络
贝叶斯网络有个重要知识点：P(a,b|c) = P(a|c)*P(b|c), 即在C给定的条件下，a和b被阻断独立的

---

【EM】
- EM是算法，不是专门用来做回归和分类的，它是在其他算法中充当迭代的功能，用来聚类的
- GMM 我们知道男的高斯分布，知道女的高斯分布，他们在图形投影上有重合，现在算p(x,y),很难算，引入GMM

Nk总影响

聚类没有用到y,所以可以用来预测？y_hat,案例二。其实不分训练和测试集，都是训练集？
聚合的结果非常好，但是参数即方差很大，这是不合理的

极大似然函数就是用来求参数的

【贝叶斯】
- pd.Categorical().codes机器学习数据预处理常用，比如男女变01
one-hot编码一般是深度学习用的，比如0001.0010
- 后验概率和条件概率的区别：首先事件发生是有先后的，后验概率是我们要求的。比如已知各种类别P(A)的概率，已知各种类别A情况下的特征B，即P(B|A)，现在给定一系列特征B，求它是哪种类别，即求P(A|B)
- 朴素贝叶斯算法：要进行思想的转换，即用贝叶斯定理来进行分类
- 整个贝叶斯求的就是后验概率：通过先验概率和条件概率求后验概率
- 因为样本的分布有特征，比如x1概率是0.01，x2的是0.99，那么X1的影响极其小，我们不能简单的按比例计算概率，才按特征引入了高斯、伯努利和多项式朴素贝叶斯
- 贝叶斯在文本挖掘用的比较多
- 怎么判断特征是否独立：用cov(x,y) (不需要判断，只是假设。如果不独立，有其他办法?)

- 卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。

- 文本处理：用TF/DF的值来作为单词的权重，文档A中各个单词出现的频率TF(term/token)，文档A中的各个单词在所有文档中出现的频率DF(出现的文档/总的文档)

40 逻辑回归和梯度下降
sigmoid能把任意输入转化为0到1的值，默认阈值是0.5，我们可以认为大于0.5属于1，小于0.5等于0，阈值可以自己设置。这样相当间接的把回归的问题转换为分类问题（回归算法中算法的最终结果是一个连续的数据值）。所以逻辑回归做的不是回归问题，是分类问题，但是是二分类。

极大似然估计，就是求一套斯塔，使的斯塔和x组合，使求的结果和真实的y最接近，即误差最小。误差最小，用的最小二乘法。

梯度下降：回归中求最小误差，我们取对数后对斯塔求导，使函数为0，但是我们化简后发现还是找不出斯塔值，使函数等于0，这个时候引出梯度下降

目的：找全局最小值
简单方法：斯塔设置一个可能的范围，for循环代入函数，得出这个范围函数的最小值，但是可能是局部最小值。而且我们求的不是一个斯塔，而是一套斯塔，各种组合计算复杂，又不科学
那我们可以利用坡度的反方向（负梯度下降），坡度为负，坡度越大，下降越快（就是对各个点求导）。然后引入阿尔法，就是步长或学习率的意思。但是步长过大，很可能一步过大，直接跨过最优解。所以学习率一般设置的小些，然后不断的试，求出斯塔


在梯度下降的公式中中，斯塔和阿尔法都是已知的，斯塔可以初始化0，阿尔法设置一个教小数，
批量下降的思想中，斯塔迭代更新一次，是要计算全部数据一次的，耗时慢

逻辑回归的求解是求一套斯塔，即一个向量θn，softmax多分类求的却是一个二维矩阵θk*n

46 ROC指标与测试集的价值
【ROC代码？样本真实值？概率？根据不同阈值划出的ROC曲线？】

47 交叉验证
除了手动把数据集等分，依次计算获取score然后算平均值
还可以直接使用 sklearn.cross_validation的KFold和cross_val_score函数

48 多类别
逻辑回归把输入转换为概率值(sigmoid函数)，然后设置一个阈值，大于多少等于1， 0/1分类
针对的是二分类，多分类怎么办？one vs all
abc类，先分为A类和非A类，然后B类和非B类，最后C类和非C类，即三个二分类完成一个三分类
然后讲了pd.get_dummies函数，对数据预处理，将年份等数据转换为，比如001 010 102
【熟悉LogisticRegressionCV（sigmoid函数是怎么体现的）、多分类】

49-51 K-近邻算法
思想：物以类聚，人以群分
不需要训练数据集，训练时间复杂度为0，即平时不干活，一来新数据，直接根据旧数据进行分类。
Knn的分类涉及距离计算，计算复杂度和数据大小正相关，数据越大越慢，分类效率低下
重点：如果样本某个类别数量占99%，那对分类结果影响太大，所以不同样本类别赋予不同权重
另外如果有些邻居很近，有些很远，分类的时候都按一票来投，不公平，所以近的邻居加权
【K近邻怎么做回归？和决策回归树一样，将最近邻居属性算平均赋值给样本，即得到该样本属性】

58 随机森林的重要性
做特征选择？【没看完，只知道过程中涉及特征选择】
也可以做多目标的分类，不需要做额外的计算

k-means为什么对初始值敏感？
具体是怎么根据距离聚类的？

回归模型经常用stand  分类模型用minmax
先逻辑回归，再随机森林，还不行用提升算法算